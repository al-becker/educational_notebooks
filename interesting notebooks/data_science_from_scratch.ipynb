{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "# Linear Algebra\n",
    "\n",
    "## Vectors\n",
    "Vector is a geometric object that has magnitude (or length) and direction. Vectors can be added to other vectors according to vector algebra. Concretely, vectors are points in some finite-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "Vector = List[float]\n",
    "\n",
    "height_weight_age = [70,  # inches,\n",
    "                     170, # pounds,\n",
    "                     40]  # years\n",
    "\n",
    "grades = [95, # exam1\n",
    "          80, # exam2\n",
    "          75, # exam3\n",
    "          62] # exam4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 7, 9]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add(v: Vector, w: Vector) -> Vector:\n",
    "    \"\"\"Adds corresponding elements\"\"\"\n",
    "    assert len(v) == len(w), \"vectors must be the same length\"\n",
    "    \n",
    "    return [v_i + w_i for v_i, w_i in zip(v, w)]\n",
    "\n",
    "\n",
    "add([1,2,3],[4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-3, -3, -3]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def subtract(v: Vector, w: Vector) -> Vector:\n",
    "    \"\"\"Subtracts corresponding elements\"\"\"\n",
    "    assert len(v) == len(w), \"vectors must be the same length\"\n",
    "    \n",
    "    return [v_i - w_i for v_i, w_i in zip(v, w)]\n",
    "\n",
    "\n",
    "subtract([1,2,3],[4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 20]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vector_sum(vectors: List[Vector]) -> Vector:\n",
    "    \"\"\"Sums all corresponding elements\"\"\"\n",
    "    assert vectors, \"no vectors provided!\"\n",
    "    \n",
    "    num_elements = len(vectors[0])\n",
    "    assert all(len(v) == num_elements for v in vectors), \"different sizes!\"\n",
    "    \n",
    "    return [sum(vector[i] for vector in vectors) for i in range(num_elements)]\n",
    "\n",
    "\n",
    "vector_sum([[1,2],[3,4],[5,6],[7,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scalar_multiply(c: float, v: Vector) -> Vector:\n",
    "    \"\"\"Multiplies every element by c\"\"\"\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "\n",
    "scalar_multiply(2,[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.0, 4.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vector_mean(vectors: List[Vector]) -> Vector:\n",
    "    \"\"\"Computes the element-wise average\"\"\"\n",
    "    n = len(vectors)\n",
    "    return scalar_multiply(1/n, vector_sum(vectors))\n",
    "\n",
    "\n",
    "vector_mean([[1,2],[3,4],[5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dot(v: Vector, w: Vector) -> float:\n",
    "    \"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    assert len(v) == len(w), \"vectos must be same length\"\n",
    "    \n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v,w))\n",
    "\n",
    "\n",
    "dot([1, 2, 3], [4, 5, 6]) # 1*4 + 2*5 + 3*6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If w has magnitude 1, the dot product measures how far the vector v extends in the w direction. For example, if w = [1, 0], then dot(v,w) is just the first component of v. Another way of saying this is that it's the length of the vector you'd get if you projected v onto w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum_of_squares(v: Vector) -> float:\n",
    "    \"\"\"Returns v_1 * v_1 + ... + v_n + v_n\"\"\"\n",
    "    return dot(v,v)\n",
    "\n",
    "\n",
    "sum_of_squares([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def magnitude(v: Vector) -> float:\n",
    "    \"\"\"Returns the magnitude (or length) of v\"\"\"\n",
    "    return math.sqrt(sum_of_squares(v)) # math.sqrt is square root funtion\n",
    "\n",
    "\n",
    "magnitude([3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the distance between two vectors: $$\\sqrt{(v_1 - w_1)^2 + ... + (v_n - w_n)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def squared_distance(v: Vector, w: Vector) -> Vector:\n",
    "    \"\"\"Computes (v_1 - w_1) ** 2 + ... + (v_n - w_n) ** 2\"\"\"\n",
    "    return sum_of_squares(subtract(v,w))\n",
    "\n",
    "def distance(v: Vector, w: Vector) -> Vector:\n",
    "    \"\"\"Computes the distance between v and w\"\"\"\n",
    "    return math.sqrt(squared_distance(v,w))\n",
    "\n",
    "#alternative\n",
    "def distance(v: Vector, w: Vector) -> Vector:\n",
    "    return magnitude(subtract(v,w))\n",
    "\n",
    "distance([3,4],[9,12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix = List[List[float]]\n",
    "\n",
    "A = [[1,2,3],\n",
    "     [4,5,6]]\n",
    "\n",
    "B = [[1,2],\n",
    "     [3,4],\n",
    "     [5,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def shape(A: Matrix) -> Tuple[int,int]:\n",
    "    \"\"\"Returns (# of rows of A, # of columns of A)\"\"\"\n",
    "    num_rows = len(A)\n",
    "    num_columns = len(A[0]) if A else 0\n",
    "    return num_rows, num_columns\n",
    "\n",
    "shape(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 1]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_row(A: Matrix, i: int) -> Vector:\n",
    "    \"\"\"Returns the i-th row of A (as a Vector)\"\"\"\n",
    "    return A[i]\n",
    "\n",
    "def get_column(A: Matrix, j: int) -> Vector:\n",
    "    \"\"\"Returns the j-th column of A (as a Vector)\"\"\"\n",
    "    return [A_i[j] for A_i in A]\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "def make_matrix(num_rows: int,\n",
    "                num_cols: int,\n",
    "                entry_fn: Callable[[int, int], float]) -> Matrix:\n",
    "    \"\"\"Returns a num_rows x num_cols matrix whose (i,j)-th entry is entry_fn(i,j)\"\"\"\n",
    "    return [[entry_fn(i,j) for j in range(num_cols)] for i in range(num_rows)]\n",
    "\n",
    "def identity_matrix(n: int) -> Matrix:\n",
    "    \"\"\"Returns the n x n identity matrix\"\"\"\n",
    "    return make_matrix(n,n, lambda i, j: 1 if i==j else 0)\n",
    "\n",
    "identity_matrix(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning we frequently will need to maximize or minimize functions that take as input a vector of real numbers and output a single real number. For functions like this, the gradient (if you remember your calculus, this is the vector of partial derivatives) gives the direction in which the function most quickly increases. Accordingly, one approach to maximize a function is to take a pick a random starting point, compute the gradient, take a small step in the direction of the gradient (i.e., the direction that causes the function to increase the most), and repeat with the new starting point. Similarly, you can try to minimize a function by taking small steps in the opposite direction.\n",
    "\n",
    "If a function has a unique global minimum, this procedure is likely to find it. If a function has multiple (local) minima, this procedure might \"find\" the wrong one of them, in which case you might rerun the procedure from different starting points. If a function has no minimum, then it's possible the procedure might go on forever.\n",
    "\n",
    "Wikipedia: In numerical mathematics, numerical differentiation is the approximate calculation of the derivative from given function values, usually by means of a difference quotient. This is necessary if the derivative function is not given or the function itself is only available indirectly, for example via measured values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def difference_quotient(f: Callable[[float], float],\n",
    "                        x: float,\n",
    "                        h: float) -> float:\n",
    "    return (f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When f is a function of many variables, it has multiple partial derivatives, each indicating how f changes when we make small changes in just one of the input variables. We calculate its ith partial derivative by treating it as a function of just its ith variable, holding the other variables fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f: Callable[[Vector], float],\n",
    "                                v: Vector,\n",
    "                                i: int,\n",
    "                                h: float) -> float:\n",
    "    \"\"\"Returns the i-th partial difference quotient of f at v\"\"\"\n",
    "    w = [v_j + (h if j == i else 0) for j, v_j in enumerate(v)]\n",
    "    return (f(w) - f(v)) / h\n",
    "\n",
    "def estimate_gradient(f: Callable[[Vector], float],\n",
    "                      v: Vector,\n",
    "                      h: float = 0.0001):\n",
    "    return [partial_difference_quotient(f, v, i, h) for i in range(len(v))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Gradient\n",
    "\n",
    "To compute the minimum for the sum_of_squares function (obviously it is 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector:\n",
    "    \"\"\"Moves 'step_size' in the 'gradient' direction from 'v'\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)\n",
    "\n",
    "# to compute the gradient directly\n",
    "def sum_of_squares_gradient(v: Vector) -> Vector:\n",
    "    return [2 * v_i for v_i in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1.3275126616204336, 2.0471852377664312, -9.471442186692585]\n",
      "100 [0.17601077060707127, 0.2714534280450831, -1.2561418255044021]\n",
      "200 [0.02329910120839225, 0.03595666405124445, -0.16663234006143507]\n",
      "300 [0.0030465474327986963, 0.004725187795727537, -0.022142075958856148]\n",
      "400 [0.0003606627453448042, 0.0005832832847834357, -0.002979841302456668]\n",
      "500 [4.461910910148e-06, 3.3985747983549746e-05, -0.0004385542523739083]\n",
      "600 [-4.2777285561916555e-05, -3.8861847400932966e-05, -0.0001015298923908456]\n",
      "700 [-4.904212681886677e-05, -4.852286314882374e-05, -5.683387144417837e-05]\n",
      "800 [-4.987296728411462e-05, -4.9804102766801246e-05, -5.090630499596878e-05]\n",
      "900 [-4.998315297763518e-05, -4.9974020195932166e-05, -5.012019376607058e-05]\n"
     ]
    }
   ],
   "source": [
    "# pick a random starting point\n",
    "v = [random.uniform(-10, 10) for i in range(3)]\n",
    "\n",
    "for epoch in range(1000):\n",
    "    grad = estimate_gradient(sum_of_squares, v)\n",
    "    v = gradient_step(v, grad, -0.01)\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a certain point we stop improving our prediction. The further increase the prediction we can either reduce the stepsize or reduce h since we can no longer optimaly predict the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Gradient Descent to Fit Models\n",
    "\n",
    "Example: In this case we know the parameters of the linear relationship between x and y, but imagine we'd like to learn them from the data. We'll use gradient descent to find the slope and intercept that minimize the average squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [33.01363393477094, 0.6849026993548983]\n",
      "200 [19.99826639873727, 2.1135528563548363]\n",
      "400 [19.998838257856573, 3.0656986334551553]\n",
      "600 [19.99922147910432, 3.7037622411154745]\n",
      "800 [19.99947828802765, 4.131349252697213]\n",
      "1000 [19.9996503839735, 4.417889105900587]\n",
      "1200 [19.99976571101975, 4.609908707174461]\n",
      "1400 [19.999842995394648, 4.7385872377913]\n",
      "1600 [19.999894786147962, 4.824818873165302]\n",
      "1800 [19.999929492802867, 4.882605474423722]\n",
      "2000 [19.999952750852177, 4.921330140499186]\n",
      "2200 [19.99996833682148, 4.947280788746358]\n",
      "2400 [19.99997878148241, 4.964671155473743]\n",
      "2600 [19.999985780786716, 4.9763250013442875]\n",
      "2800 [19.999990471246374, 4.9841346194911225]\n",
      "3000 [19.999993614474736, 4.989368096600474]\n",
      "3200 [19.99999572085348, 4.99287521847751]\n",
      "3400 [19.999997132405845, 4.995225454009914]\n",
      "3600 [19.99999807833263, 4.9968004226740845]\n",
      "3800 [19.999998712228688, 4.997855859994694]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[19.999999135294, 4.9985602639912114]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ranges from - 50 to 49, y is always 20 * x + 5\n",
    "inputs = [(x, 20 * x + 5) for x in range(-50, 50)]\n",
    "\n",
    "def linear_gradient(x: float, y: float, theta: Vector) -> Vector:\n",
    "    slope, intercept = theta\n",
    "    predicted = slope * x + intercept\n",
    "    error = (predicted - y)\n",
    "    squared_error = error ** 2\n",
    "    grad = [2 * error * x, 2 * error]\n",
    "    return grad\n",
    "\n",
    "# random values for slope and intercept\n",
    "theta = [random.uniform(-1,1), random.uniform(-1,1)]\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(4000):\n",
    "    grad = vector_mean([linear_gradient(x,y,theta) for x,y in inputs])\n",
    "    theta = gradient_step(theta,grad,-learning_rate)\n",
    "    if epoch % 200 == 0:\n",
    "        print(epoch, theta)\n",
    "        \n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minibatch and Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar, Iterator\n",
    "\n",
    "T = TypeVar('T')\n",
    "\n",
    "def minibatches(dataset: List[T],\n",
    "                batch_size: int,\n",
    "                shuffle: bool = True) -> Iterator[List[T]]:\n",
    "    \"\"\"Generates 'batch_size'-sized minibatches from the dataset\"\"\"\n",
    "    batch_starts = [start for start in range(0,len(dataset),batch_size)]\n",
    "    \n",
    "    if shuffle: random.shuffle(batch_starts)\n",
    "        \n",
    "    for start in batch_starts:\n",
    "        end = start + batch_size\n",
    "        yield dataset[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [18.203900463218186, 2.811606844794329]\n",
      "100 [20.066553626911066, 4.194745136887922]\n",
      "200 [19.980792220761987, 4.812616296384315]\n",
      "300 [19.998916893683376, 4.969689299743435]\n",
      "400 [20.00002304142001, 4.995348010220143]\n",
      "500 [20.00011692020163, 4.99922104140855]\n",
      "600 [19.999988356428258, 4.9998100075877865]\n",
      "700 [19.99999837165458, 4.999954736021836]\n",
      "800 [20.000000466918248, 4.9999913886067615]\n",
      "900 [19.999999625638402, 4.9999976921368585]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[19.999999971038772, 4.999999355730051]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for batch in minibatches(inputs, batch_size=20):\n",
    "        grad = vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    if epoch%100 == 0: print(epoch, theta)\n",
    "\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [20.11814367851677, -0.8802633514541575]\n",
      "10 [20.07620159805196, 1.2072618226666982]\n",
      "20 [20.04914961855205, 2.5537043050753008]\n",
      "30 [20.031701236915577, 3.422152454694023]\n",
      "40 [20.02044712639024, 3.9822968324655155]\n",
      "50 [20.01318828939189, 4.3435869383461725]\n",
      "60 [20.008506374841414, 4.576617110795876]\n",
      "70 [20.00548656486774, 4.726920316536092]\n",
      "80 [20.003538798249018, 4.823865075401264]\n",
      "90 [20.002282508168367, 4.886393922132545]\n"
     ]
    }
   ],
   "source": [
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "for epoch in range(100):\n",
    "    for x, y in inputs:\n",
    "        grad = linear_gradient(x, y, theta)\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    if epoch%10 == 0: print(epoch, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
